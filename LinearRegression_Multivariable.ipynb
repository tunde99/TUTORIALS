{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LinearRegression_Multivariable.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNn7XC5C3X9OIMNmyHevnQK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tunde99/TUTORIALS/blob/main/LinearRegression_Multivariable.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eYLG1MM8BhCJ",
        "outputId": "32e29250-e0fc-4051-c0ee-b19c8275e6fd"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.datasets import load_boston\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "X: design matrix of shape (m,n)\n",
        "    m --> #training examples\n",
        "    n --> #features\n",
        "y: label of shape (m,1)\n",
        "w: weight vector of shape (n,1)\n",
        "b: bias term - a scaler\n",
        "\"\"\"\n",
        "class LinearRegressionModel():\n",
        "    def __init__(self, X, learning_rate, epochs):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.epochs = epochs\n",
        "        self.m,self.n = X.shape\n",
        "    \n",
        "    #1 Initialise parameters\n",
        "    def initialise_parameters(self):\n",
        "        np.random.seed(1)\n",
        "        self.w = np.random.randn(self.n,1)\n",
        "        self.b = 0\n",
        "        return self.w, self.b\n",
        "\n",
        "    #2 compute forward\n",
        "    def forward_propagation(self, X):\n",
        "        z = np.dot(self.w.T, X.T) + self.b\n",
        "        return z\n",
        "\n",
        "    #3 compute cost\n",
        "    def cost_function(self, y, y_hat):\n",
        "        cost = 1/2*self.m * np.sum((y_hat - y) ** 2)\n",
        "        return cost\n",
        "\n",
        "    #4 compute backward\n",
        "    def backward_propagation(self, X, y, y_hat):\n",
        "        self.dw = 1/self.m * np.sum(np.dot(X.T, (y_hat - y)))\n",
        "        self.db = 1/self.m * np.sum(y_hat - y)\n",
        "        return self.dw, self.db\n",
        "\n",
        "    #5 update gradient descent\n",
        "    def gradient_update(self):\n",
        "        self.w = self.w - self.learning_rate*self.dw\n",
        "        self.b = self.b - self.learning_rate*self.db\n",
        "        return self.w, self.b\n",
        "\n",
        "    #6 training\n",
        "    def train(self, X, y):\n",
        "        #1\n",
        "        self.w, self.b = self.initialise_parameters()\n",
        "\n",
        "        for epoch in range(1, self.epochs+1):\n",
        "            \n",
        "            #2\n",
        "            y_hat = self.forward_propagation(X)\n",
        "            \n",
        "            #3\n",
        "            train_cost = self.cost_function(y_train, y_hat)\n",
        "            val_cost = self.cost_function(y_val,y_hat)\n",
        "            \n",
        "            #4\n",
        "            self.dw, self.db = self.backward_propagation(X, y, y_hat)\n",
        "            self.w, self.b = self.gradient_update()\n",
        "\n",
        "            if epoch%10 == 0:\n",
        "                print(f'Epoch {epoch} / {self.epochs}')\n",
        "                print(f'Training Cost: {train_cost} | Valid Cost: {val_cost}')\n",
        "            \n",
        "        return self.w, self.b\n",
        "\n",
        "\n",
        "    def predict(self, X):\n",
        "        y_hat = self.forward_propagation(X)\n",
        "        return y_hat\n",
        "\n",
        "    def evaluate(self, y_hat, y):\n",
        "        mae = 1/self.m * np.sum(np.abs(y_hat == y))\n",
        "        return mae\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    np.random.seed(2)\n",
        "    boston_data = load_boston()\n",
        "    boston = pd.DataFrame(boston_data['data'])\n",
        "    boston.columns = boston_data['feature_names']\n",
        "   \n",
        "    X = (boston - boston.mean())/(boston.max() - boston.min())\n",
        "    y = boston_data['target']\n",
        "    y = y.reshape(-1,1) # y = y[:,newaxis]\n",
        "\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.33, random_state=5)\n",
        "   \n",
        "    lin_reg = LinearRegressionModel(X_train, 0.1, 100)\n",
        "    w, b = lin_reg.train(X_train,y_train)\n",
        "    y_hat = lin_reg.predict(X_val)\n",
        "    mae = lin_reg.evaluate(y_hat, y_val)\n",
        "\n",
        "    print(f'MAE: {mae}')\n",
        "\n",
        "        "
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 10 / 100\n",
            "Training Cost: 2.1051941831408305e+37 | Valid Cost: 1.0370720607212944e+37\n",
            "Epoch 20 / 100\n",
            "Training Cost: 4.755782598072592e+67 | Valid Cost: 2.342819155982663e+67\n",
            "Epoch 30 / 100\n",
            "Training Cost: 1.0743649351334509e+98 | Valid Cost: 5.292594223223785e+97\n",
            "Epoch 40 / 100\n",
            "Training Cost: 2.4270663976778515e+128 | Valid Cost: 1.1956344790920392e+128\n",
            "Epoch 50 / 100\n",
            "Training Cost: 5.482914702540293e+158 | Valid Cost: 2.7010228770626224e+158\n",
            "Epoch 60 / 100\n",
            "Training Cost: 1.2386292218496894e+189 | Valid Cost: 6.101801771353928e+188\n",
            "Epoch 70 / 100\n",
            "Training Cost: 2.7981510427458536e+219 | Valid Cost: 1.378440189199285e+219\n",
            "Epoch 80 / 100\n",
            "Training Cost: 6.3212211692595195e+249 | Valid Cost: 3.113993909340235e+249\n",
            "Epoch 90 / 100\n",
            "Training Cost: 1.4280085835353503e+280 | Valid Cost: 7.034732550159394e+279\n",
            "Epoch 100 / 100\n",
            "Training Cost: inf | Valid Cost: inf\n",
            "MAE: 0.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/numpy/core/fromnumeric.py:87: RuntimeWarning: overflow encountered in reduce\n",
            "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:38: RuntimeWarning: overflow encountered in double_scalars\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vB2MQbBcK920"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}